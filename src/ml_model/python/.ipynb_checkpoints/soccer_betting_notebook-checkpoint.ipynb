{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Modeling with heterogeneous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/images/predictive_modeling_data_flow.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading tabular data from the Titanic kaggle challenge in a pandas Data Frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us have a look at the Titanic dataset from the Kaggle Getting Started challenge at:\n",
    "\n",
    "https://www.kaggle.com/c/titanic-gettingStarted\n",
    "\n",
    "We can load the CSV file as a pandas data frame in one line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bit0,bit1,bit2,bit3,bit4,bit5,bit6,bit7,bit8,bit9,bit10,bit11,bit12,bit13,bit14,bit15,B365H,B365D,B365A,BWH,BWD,BWA,GBH,GBD,GBA,IWH,IWD,IWA,LBH,LBD,LBA,WHH,WHD,WHA,VCH,VCD,VCA,result\n",
      "0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,1.3,5,9.5,1.35,4.2,8,1.36,4.25,9,1.35,4,7.5,1.33,4,8,1.33,4,8,1.35,4,8,1\n",
      "0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,2.75,3.25,2.25,2.85,3.35,2.2,3,3.35,2.25,2.7,3,2.3,3,3.2,2.1,2.87,3.25,2.15,2.7,3.25,2.25,2\n",
      "0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,2.1,3.25,3,1.95,3.3,3.5,2.05,3.25,3.5,1.9,3.1,3.5,2,3.2,3.2,1.95,3.25,3.3,1.85,3.3,3.6,1\n",
      "0,0,0,0,0,0,1,1,0,0,0,0,1,1,1,1,1.57,3.4,5.5,1.55,3.7,5.35,1.6,3.65,5.5,1.55,3.5,4.8,1.53,3.4,5.5,1.57,3.5,5,1.55,3.5,5,1\n"
     ]
    }
   ],
   "source": [
    "#!curl -s https://dl.dropboxusercontent.com/u/5743203/data/titanic/titanic_train.csv | head -5\n",
    "with open('../../../data/clean_data.csv', 'r') as f:\n",
    "    for i, line in zip(range(5), f):\n",
    "        print(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data = pd.read_csv('https://dl.dropboxusercontent.com/u/5743203/data/titanic/titanic_train.csv')\n",
    "data = pd.read_csv('../../../data/clean_data.csv')  # result column, 1 means home win, 2 home draw, 3 home lose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas data frames have a HTML table representation in the IPython notebook. Let's have a look at the first 5 rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bit0</th>\n",
       "      <th>bit1</th>\n",
       "      <th>bit2</th>\n",
       "      <th>bit3</th>\n",
       "      <th>bit4</th>\n",
       "      <th>bit5</th>\n",
       "      <th>bit6</th>\n",
       "      <th>bit7</th>\n",
       "      <th>bit8</th>\n",
       "      <th>bit9</th>\n",
       "      <th>bit10</th>\n",
       "      <th>bit11</th>\n",
       "      <th>bit12</th>\n",
       "      <th>bit13</th>\n",
       "      <th>bit14</th>\n",
       "      <th>bit15</th>\n",
       "      <th>B365H</th>\n",
       "      <th>B365D</th>\n",
       "      <th>B365A</th>\n",
       "      <th>BWH</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 1</td>\n",
       "      <td> 1</td>\n",
       "      <td> 1</td>\n",
       "      <td> 0</td>\n",
       "      <td> 1.30</td>\n",
       "      <td> 5.00</td>\n",
       "      <td> 9.50</td>\n",
       "      <td> 1.35</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 1</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 1</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 1</td>\n",
       "      <td> 2.75</td>\n",
       "      <td> 3.25</td>\n",
       "      <td> 2.25</td>\n",
       "      <td> 2.85</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 1</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 1</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 2.10</td>\n",
       "      <td> 3.25</td>\n",
       "      <td> 3.00</td>\n",
       "      <td> 1.95</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 1</td>\n",
       "      <td> 1</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 1</td>\n",
       "      <td> 1</td>\n",
       "      <td> 1</td>\n",
       "      <td> 1</td>\n",
       "      <td> 1.57</td>\n",
       "      <td> 3.40</td>\n",
       "      <td> 5.50</td>\n",
       "      <td> 1.55</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 1</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 1</td>\n",
       "      <td> 0</td>\n",
       "      <td> 1</td>\n",
       "      <td> 1</td>\n",
       "      <td> 2.40</td>\n",
       "      <td> 3.20</td>\n",
       "      <td> 2.60</td>\n",
       "      <td> 2.50</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   bit0  bit1  bit2  bit3  bit4  bit5  bit6  bit7  bit8  bit9  bit10  bit11  \\\n",
       "0     0     0     0     0     0     0     0     0     0     0      0      0   \n",
       "1     0     0     0     0     0     0     0     1     0     0      0      1   \n",
       "2     0     0     0     0     0     0     1     0     0     0      0      1   \n",
       "3     0     0     0     0     0     0     1     1     0     0      0      0   \n",
       "4     0     0     0     0     0     1     0     0     0     0      0      0   \n",
       "\n",
       "   bit12  bit13  bit14  bit15  B365H  B365D  B365A   BWH      \n",
       "0      1      1      1      0   1.30   5.00   9.50  1.35 ...  \n",
       "1      0      0      0      1   2.75   3.25   2.25  2.85 ...  \n",
       "2      0      0      0      0   2.10   3.25   3.00  1.95 ...  \n",
       "3      1      1      1      1   1.57   3.40   5.50  1.55 ...  \n",
       "4      1      0      1      1   2.40   3.20   2.60  2.50 ...  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bit0      17023\n",
       "bit1      17023\n",
       "bit2      17023\n",
       "bit3      17023\n",
       "bit4      17023\n",
       "bit5      17023\n",
       "bit6      17023\n",
       "bit7      17023\n",
       "bit8      17023\n",
       "bit9      17023\n",
       "bit10     17023\n",
       "bit11     17023\n",
       "bit12     17023\n",
       "bit13     17023\n",
       "bit14     17023\n",
       "bit15     17023\n",
       "B365H     17023\n",
       "B365D     17023\n",
       "B365A     17023\n",
       "BWH       17023\n",
       "BWD       17023\n",
       "BWA       17023\n",
       "GBH       17023\n",
       "GBD       17023\n",
       "GBA       17023\n",
       "IWH       17023\n",
       "IWD       17023\n",
       "IWA       17023\n",
       "LBH       17023\n",
       "LBD       17023\n",
       "LBA       17023\n",
       "WHH       17023\n",
       "WHD       17023\n",
       "WHA       17023\n",
       "VCH       17023\n",
       "VCD       17023\n",
       "VCA       17023\n",
       "result    17023\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data frame has 891 rows. Some passengers have missing information though: in particular Age and Cabin info can be missing. The meaning of the columns is explained on the challenge website:\n",
    "\n",
    "https://www.kaggle.com/c/titanic-gettingStarted/data\n",
    "\n",
    "A data frame can be converted into a numpy array by calling the `values` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bit0',\n",
       " 'bit1',\n",
       " 'bit2',\n",
       " 'bit3',\n",
       " 'bit4',\n",
       " 'bit5',\n",
       " 'bit6',\n",
       " 'bit7',\n",
       " 'bit8',\n",
       " 'bit9',\n",
       " 'bit10',\n",
       " 'bit11',\n",
       " 'bit12',\n",
       " 'bit13',\n",
       " 'bit14',\n",
       " 'bit15',\n",
       " 'B365H',\n",
       " 'B365D',\n",
       " 'B365A',\n",
       " 'BWH',\n",
       " 'BWD',\n",
       " 'BWA',\n",
       " 'GBH',\n",
       " 'GBD',\n",
       " 'GBA',\n",
       " 'IWH',\n",
       " 'IWD',\n",
       " 'IWA',\n",
       " 'LBH',\n",
       " 'LBD',\n",
       " 'LBA',\n",
       " 'WHH',\n",
       " 'WHD',\n",
       " 'WHA',\n",
       " 'VCH',\n",
       " 'VCD',\n",
       " 'VCA',\n",
       " 'result']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17023, 38)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  0.  ,  0.  , ...,  4.  ,  8.  ,  1.  ],\n",
       "       [ 0.  ,  0.  ,  0.  , ...,  3.25,  2.25,  2.  ],\n",
       "       [ 0.  ,  0.  ,  0.  , ...,  3.3 ,  3.6 ,  1.  ],\n",
       "       ..., \n",
       "       [ 1.  ,  0.  ,  1.  , ...,  3.7 ,  2.2 ,  1.  ],\n",
       "       [ 1.  ,  1.  ,  0.  , ...,  3.7 ,  3.5 ,  2.  ],\n",
       "       [ 1.  ,  0.  ,  1.  , ...,  3.5 ,  3.2 ,  3.  ]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However this cannot be directly fed to a scikit-learn model:\n",
    "\n",
    "\n",
    "- the target variable (survival) is mixed with the input data\n",
    "\n",
    "- some attribute such as unique ids have no predictive values for the task\n",
    "\n",
    "- the values are heterogeneous (string labels for categories, integers and floating point numbers)\n",
    "\n",
    "- some attribute values are missing (nan: \"not a number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting survival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the challenge is to predict whether a passenger has survived from others known attribute. Let us have a look at the `Survived` columns:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`data.Survived` is an instance of the pandas `Series` class with an integer dtype:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `data` object is an instance pandas `DataFrame` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)\n",
    "result_column = data['result']\n",
    "type(result_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Series` can be seen as homegeneous, 1D columns. `DataFrame` instances are heterogenous collections of columns with the same length.\n",
    "\n",
    "The original data frame can be aggregated by counting rows for each possible value of the `Survived` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bit0</th>\n",
       "      <th>bit1</th>\n",
       "      <th>bit2</th>\n",
       "      <th>bit3</th>\n",
       "      <th>bit4</th>\n",
       "      <th>bit5</th>\n",
       "      <th>bit6</th>\n",
       "      <th>bit7</th>\n",
       "      <th>bit8</th>\n",
       "      <th>bit9</th>\n",
       "      <th>bit10</th>\n",
       "      <th>bit11</th>\n",
       "      <th>bit12</th>\n",
       "      <th>bit13</th>\n",
       "      <th>bit14</th>\n",
       "      <th>bit15</th>\n",
       "      <th>B365H</th>\n",
       "      <th>B365D</th>\n",
       "      <th>B365A</th>\n",
       "      <th>BWH</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>result</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td> 7974</td>\n",
       "      <td> 7974</td>\n",
       "      <td> 7974</td>\n",
       "      <td> 7974</td>\n",
       "      <td> 7974</td>\n",
       "      <td> 7974</td>\n",
       "      <td> 7974</td>\n",
       "      <td> 7974</td>\n",
       "      <td> 7974</td>\n",
       "      <td> 7974</td>\n",
       "      <td> 7974</td>\n",
       "      <td> 7974</td>\n",
       "      <td> 7974</td>\n",
       "      <td> 7974</td>\n",
       "      <td> 7974</td>\n",
       "      <td> 7974</td>\n",
       "      <td> 7974</td>\n",
       "      <td> 7974</td>\n",
       "      <td> 7974</td>\n",
       "      <td> 7974</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td> 4418</td>\n",
       "      <td> 4418</td>\n",
       "      <td> 4418</td>\n",
       "      <td> 4418</td>\n",
       "      <td> 4418</td>\n",
       "      <td> 4418</td>\n",
       "      <td> 4418</td>\n",
       "      <td> 4418</td>\n",
       "      <td> 4418</td>\n",
       "      <td> 4418</td>\n",
       "      <td> 4418</td>\n",
       "      <td> 4418</td>\n",
       "      <td> 4418</td>\n",
       "      <td> 4418</td>\n",
       "      <td> 4418</td>\n",
       "      <td> 4418</td>\n",
       "      <td> 4418</td>\n",
       "      <td> 4418</td>\n",
       "      <td> 4418</td>\n",
       "      <td> 4418</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td> 4631</td>\n",
       "      <td> 4631</td>\n",
       "      <td> 4631</td>\n",
       "      <td> 4631</td>\n",
       "      <td> 4631</td>\n",
       "      <td> 4631</td>\n",
       "      <td> 4631</td>\n",
       "      <td> 4631</td>\n",
       "      <td> 4631</td>\n",
       "      <td> 4631</td>\n",
       "      <td> 4631</td>\n",
       "      <td> 4631</td>\n",
       "      <td> 4631</td>\n",
       "      <td> 4631</td>\n",
       "      <td> 4631</td>\n",
       "      <td> 4631</td>\n",
       "      <td> 4631</td>\n",
       "      <td> 4631</td>\n",
       "      <td> 4631</td>\n",
       "      <td> 4631</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        bit0  bit1  bit2  bit3  bit4  bit5  bit6  bit7  bit8  bit9  bit10  \\\n",
       "result                                                                      \n",
       "1       7974  7974  7974  7974  7974  7974  7974  7974  7974  7974   7974   \n",
       "2       4418  4418  4418  4418  4418  4418  4418  4418  4418  4418   4418   \n",
       "3       4631  4631  4631  4631  4631  4631  4631  4631  4631  4631   4631   \n",
       "\n",
       "        bit11  bit12  bit13  bit14  bit15  B365H  B365D  B365A   BWH      \n",
       "result                                                                    \n",
       "1        7974   7974   7974   7974   7974   7974   7974   7974  7974 ...  \n",
       "2        4418   4418   4418   4418   4418   4418   4418   4418  4418 ...  \n",
       "3        4631   4631   4631   4631   4631   4631   4631   4631  4631 ...  \n",
       "\n",
       "[3 rows x 38 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('result').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.468425071961\n",
      "0.259531222464\n",
      "0.272043705575\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(np.mean(result_column == 1))\n",
    "print(np.mean(result_column == 2))\n",
    "print(np.mean(result_column == 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this the subset of the full passengers list, about 2/3 perished in the event. So if we are to build a predictive model from this data, a baseline model to compare the performance to would be to always predict death. Such a constant model would reach around 62% predictive accuracy (which is higher than predicting at random):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas `Series` instances can be converted to regular 1D numpy arrays by using the `values` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, ..., 1, 2, 3])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = result_column.values\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, 1, 2])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a predictive model on numerical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sklearn` estimators all work with homegeneous numerical feature descriptors passed as a numpy array. Therefore passing the raw data frame will not work out of the box.\n",
    "\n",
    "Let us start simple and build a first model that only uses readily available numerical features as input, namely `data['Fare']`, `data['Pclass']` and `data['Age']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bit0</th>\n",
       "      <th>bit1</th>\n",
       "      <th>bit2</th>\n",
       "      <th>bit3</th>\n",
       "      <th>bit4</th>\n",
       "      <th>bit5</th>\n",
       "      <th>bit6</th>\n",
       "      <th>bit7</th>\n",
       "      <th>bit8</th>\n",
       "      <th>bit9</th>\n",
       "      <th>bit10</th>\n",
       "      <th>bit11</th>\n",
       "      <th>bit12</th>\n",
       "      <th>bit13</th>\n",
       "      <th>bit14</th>\n",
       "      <th>bit15</th>\n",
       "      <th>B365H</th>\n",
       "      <th>B365D</th>\n",
       "      <th>B365A</th>\n",
       "      <th>BWH</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 1</td>\n",
       "      <td> 1</td>\n",
       "      <td> 1</td>\n",
       "      <td> 0</td>\n",
       "      <td> 1.30</td>\n",
       "      <td> 5.00</td>\n",
       "      <td> 9.50</td>\n",
       "      <td> 1.35</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 1</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 1</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 1</td>\n",
       "      <td> 2.75</td>\n",
       "      <td> 3.25</td>\n",
       "      <td> 2.25</td>\n",
       "      <td> 2.85</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 1</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 1</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 2.10</td>\n",
       "      <td> 3.25</td>\n",
       "      <td> 3.00</td>\n",
       "      <td> 1.95</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 1</td>\n",
       "      <td> 1</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 1</td>\n",
       "      <td> 1</td>\n",
       "      <td> 1</td>\n",
       "      <td> 1</td>\n",
       "      <td> 1.57</td>\n",
       "      <td> 3.40</td>\n",
       "      <td> 5.50</td>\n",
       "      <td> 1.55</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 1</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 1</td>\n",
       "      <td> 0</td>\n",
       "      <td> 1</td>\n",
       "      <td> 1</td>\n",
       "      <td> 2.40</td>\n",
       "      <td> 3.20</td>\n",
       "      <td> 2.60</td>\n",
       "      <td> 2.50</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   bit0  bit1  bit2  bit3  bit4  bit5  bit6  bit7  bit8  bit9  bit10  bit11  \\\n",
       "0     0     0     0     0     0     0     0     0     0     0      0      0   \n",
       "1     0     0     0     0     0     0     0     1     0     0      0      1   \n",
       "2     0     0     0     0     0     0     1     0     0     0      0      1   \n",
       "3     0     0     0     0     0     0     1     1     0     0      0      0   \n",
       "4     0     0     0     0     0     1     0     0     0     0      0      0   \n",
       "\n",
       "   bit12  bit13  bit14  bit15  B365H  B365D  B365A   BWH      \n",
       "0      1      1      1      0   1.30   5.00   9.50  1.35 ...  \n",
       "1      0      0      0      1   2.75   3.25   2.25  2.85 ...  \n",
       "2      0      0      0      0   2.10   3.25   3.00  1.95 ...  \n",
       "3      1      1      1      1   1.57   3.40   5.50  1.55 ...  \n",
       "4      1      0      1      1   2.40   3.20   2.60  2.50 ...  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data.columns[0:]\n",
    "numerical_features = data[data.columns[0:-1]]\n",
    "numerical_features.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately some passengers do not have age information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bit0     17023\n",
       "bit1     17023\n",
       "bit2     17023\n",
       "bit3     17023\n",
       "bit4     17023\n",
       "bit5     17023\n",
       "bit6     17023\n",
       "bit7     17023\n",
       "bit8     17023\n",
       "bit9     17023\n",
       "bit10    17023\n",
       "bit11    17023\n",
       "bit12    17023\n",
       "bit13    17023\n",
       "bit14    17023\n",
       "bit15    17023\n",
       "B365H    17023\n",
       "B365D    17023\n",
       "B365A    17023\n",
       "BWH      17023\n",
       "BWD      17023\n",
       "BWA      17023\n",
       "GBH      17023\n",
       "GBD      17023\n",
       "GBA      17023\n",
       "IWH      17023\n",
       "IWD      17023\n",
       "IWA      17023\n",
       "LBH      17023\n",
       "LBD      17023\n",
       "LBA      17023\n",
       "WHH      17023\n",
       "WHD      17023\n",
       "WHA      17023\n",
       "VCH      17023\n",
       "VCD      17023\n",
       "VCA      17023\n",
       "dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_features.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use pandas `fillna` method to input the median age for those passengers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data frame is clean, we can convert it into an homogeneous numpy array of floating point values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  0.  ,  0.  , ...,  1.35,  4.  ,  8.  ],\n",
       "       [ 0.  ,  0.  ,  0.  , ...,  2.7 ,  3.25,  2.25],\n",
       "       [ 0.  ,  0.  ,  0.  , ...,  1.85,  3.3 ,  3.6 ],\n",
       "       ..., \n",
       "       [ 1.  ,  0.  ,  1.  , ...,  3.13,  3.7 ,  2.2 ],\n",
       "       [ 1.  ,  1.  ,  0.  , ...,  2.1 ,  3.7 ,  3.5 ],\n",
       "       [ 1.  ,  0.  ,  1.  , ...,  2.3 ,  3.5 ,  3.2 ]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_array = numerical_features.values\n",
    "features_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_array.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the 80% of the data for training a first model and keep 20% for computing is generalization score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features_array, target, test_size=0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13109 0.462659241742\n",
      "10595 0.43378952336\n",
      "8337 0.396665467194\n",
      "6946 0.375611862943\n",
      "5893 0.354318683183\n",
      "5048 0.334587955626\n",
      "4236 0.313503305005\n",
      "3553 0.29271038559\n",
      "2987 0.268496819551\n",
      "0.468864737847\n"
     ]
    }
   ],
   "source": [
    "def is_unexpected(features, target, threshold):\n",
    "    b365_odds = features[:, 16 : 19]\n",
    "    return (((np.amax(b365_odds, axis = 1) - np.amin(b365_odds, axis = 1)) > threshold) \n",
    "            & (np.argmin(b365_odds, axis = 1) + 1 != target))\n",
    "\n",
    "def correct_predict_based_on_odds(features, target, threshold):\n",
    "    b365_odds = features[:, 16:19]\n",
    "    new_features = features[(np.amax(b365_odds, axis = 1) - np.amin(b365_odds, axis = 1)) > threshold]\n",
    "    new_target = target[(np.amax(b365_odds, axis = 1) - np.amin(b365_odds, axis = 1)) > threshold]\n",
    "    b365_odds = b365_odds[(np.amax(b365_odds, axis = 1) - np.amin(b365_odds, axis = 1)) > threshold]\n",
    "    miss_predicts_rate = np.mean(np.argmin(b365_odds, axis = 1) + 1 != new_target)\n",
    "\n",
    "    return new_features, new_target, miss_predicts_rate\n",
    "\n",
    "def miss_predict_based_on_odds(features, target):\n",
    "    b365_odds = features[:, 16 : 19]\n",
    "    return np.argmin(b365_odds, axis = 1) + 1 != target     \n",
    "\n",
    "for threshold in np.arange(0.5, 5, 0.5):\n",
    "    new_features, new_target, miss_predicts_rate = correct_predict_based_on_odds(features_train, target_train, threshold)\n",
    "    print(len(new_features), miss_predicts_rate)\n",
    "    \n",
    "print(np.mean(miss_predict_based_on_odds(features_train, target_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13618, 37)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3405, 37)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13618,)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3405,)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler  \n",
    "scaler = StandardScaler()  \n",
    "# Don't cheat - fit only on training data\n",
    "scaler.fit(features_train)  \n",
    "X_train = scaler.transform(features_train)  \n",
    "# apply same transformation to test data\n",
    "X_test = scaler.transform(features_test)  \n",
    "# print(X_train[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a simple model from sklearn, namely `LogisticRegression`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Odds Difference Threshold to remove sample data:  0.1\n",
      "Number of training data under this odds difference threshold:  13616\n",
      "Number of test data under this odds difference threshold:  3405\n",
      "Percentage of predictions with the Prob_threshold of  0.75  :  0.0828193832599\n",
      "Accuracy with this prob_threshold:  0.783687943262\n",
      "Total accuracy:  0.517474302496\n",
      "[[ 0.    1.    1.    0.    0.    1.    0.    1.    0.    1.    1.    0.    0.\n",
      "   0.    1.    0.    1.57  3.5   6.5   1.5   3.5   6.75  1.55  3.3   6.5\n",
      "   1.6   3.3   5.5   1.62  3.2   5.5   1.5   3.4   6.    1.5   3.5   6.5 ]\n",
      " [ 1.    0.    1.    0.    0.    1.    1.    1.    1.    0.    1.    0.    1.\n",
      "   1.    1.    0.    2.1   3.3   3.5   2.1   3.2   3.2   2.1   3.25  3.4\n",
      "   2.2   3.3   3.1   1.91  3.25  3.4   2.1   3.2   3.    2.1   3.2   3.25]]\n",
      "\n",
      "\n",
      "\n",
      "Odds Difference Threshold to remove sample data:  0.6\n",
      "Number of training data under this odds difference threshold:  13078\n",
      "Number of test data under this odds difference threshold:  3252\n",
      "Percentage of predictions with the Prob_threshold of  0.75  :  0.0873308733087\n",
      "Accuracy with this prob_threshold:  0.781690140845\n",
      "Total accuracy:  0.524600246002\n",
      "[[ 1.    0.    1.    1.    1.    0.    1.    1.    1.    0.    1.    0.    1.\n",
      "   1.    1.    1.    2.2   3.3   3.25  2.15  3.2   3.3   2.15  3.25  3.25\n",
      "   2.1   3.3   3.1   2.1   3.2   3.    2.15  3.3   3.4   2.2   3.4   3.6 ]\n",
      " [ 0.    0.    0.    0.    0.    0.    1.    1.    0.    0.    0.    1.    1.\n",
      "   0.    0.    0.    1.83  3.75  4.    1.72  3.7   4.6   1.85  3.5   4.\n",
      "   1.75  3.4   4.3   1.8   3.5   4.5   1.8   3.5   4.5   1.9   3.75  4.4 ]]\n",
      "\n",
      "\n",
      "\n",
      "Odds Difference Threshold to remove sample data:  1.1\n",
      "Number of training data under this odds difference threshold:  10007\n",
      "Number of test data under this odds difference threshold:  2477\n",
      "Percentage of predictions with the Prob_threshold of  0.75  :  0.112232539362\n",
      "Accuracy with this prob_threshold:  0.776978417266\n",
      "Total accuracy:  0.563181267662\n",
      "[[ 0.    1.    1.    0.    0.    1.    0.    1.    0.    1.    1.    0.    0.\n",
      "   0.    0.    1.    1.53  3.25  7.    1.45  3.3   9.    1.5   3.25  7.5\n",
      "   1.5   3.3   6.7   1.5   3.2   7.    1.5   3.3   6.5   1.45  3.4   7.  ]\n",
      " [ 0.    1.    1.    1.    1.    1.    0.    0.    0.    1.    1.    0.    1.\n",
      "   0.    1.    0.    5.75  3.1   1.75  5.    3.3   1.72  5.5   3.25  1.7\n",
      "   5.    3.1   1.65  5.5   3.3   1.73  5.    3.2   1.8   5.5   3.12  1.75]]\n",
      "\n",
      "\n",
      "\n",
      "Odds Difference Threshold to remove sample data:  1.6\n",
      "Number of training data under this odds difference threshold:  8057\n",
      "Number of test data under this odds difference threshold:  2020\n",
      "Percentage of predictions with the Prob_threshold of  0.75  :  0.137623762376\n",
      "Accuracy with this prob_threshold:  0.76618705036\n",
      "Total accuracy:  0.589108910891\n",
      "[[  0.     1.     1.     0.     0.     1.     0.     0.     0.     1.     1.\n",
      "    0.     1.     0.     0.     0.     1.16   6.    19.     1.16   6.    14.5\n",
      "    1.17   6.    14.     1.17   5.5   11.     1.17   5.5   12.     1.16\n",
      "    5.5   12.     1.14   6.    12.  ]\n",
      " [  0.     1.     0.     1.     0.     0.     0.     0.     0.     1.     0.\n",
      "    0.     0.     1.     0.     1.     2.15   2.87   3.4    2.2    2.8\n",
      "    3.45   2.15   2.85   3.35   2.3    2.9    3.     2.25   3.     2.87\n",
      "    2.15   2.9    3.2    2.2    2.8    3.2 ]]\n",
      "\n",
      "\n",
      "\n",
      "Odds Difference Threshold to remove sample data:  2.1\n",
      "Number of training data under this odds difference threshold:  6609\n",
      "Number of test data under this odds difference threshold:  1622\n",
      "Percentage of predictions with the Prob_threshold of  0.75  :  0.168927250308\n",
      "Accuracy with this prob_threshold:  0.766423357664\n",
      "Total accuracy:  0.618988902589\n",
      "[[ 0.    1.    1.    0.    1.    1.    0.    1.    0.    1.    1.    0.    1.\n",
      "   1.    0.    0.    2.3   3.1   3.3   2.3   2.8   3.25  2.3   3.    3.35\n",
      "   2.2   3.2   3.2   2.25  3.    2.88  2.3   2.9   3.5   2.1   3.    3.5 ]\n",
      " [ 0.    0.    0.    1.    0.    1.    0.    1.    0.    0.    0.    0.    0.\n",
      "   0.    1.    1.    4.    3.4   1.9   4.    3.25  1.85  4.    3.25  1.95\n",
      "   3.3   3.2   2.    3.75  3.3   1.8   3.4   3.25  1.91  3.6   3.3   1.9 ]]\n",
      "\n",
      "\n",
      "\n",
      "Odds Difference Threshold to remove sample data:  2.6\n",
      "Number of training data under this odds difference threshold:  5752\n",
      "Number of test data under this odds difference threshold:  1400\n",
      "Percentage of predictions with the Prob_threshold of  0.75  :  0.194285714286\n",
      "Accuracy with this prob_threshold:  0.768382352941\n",
      "Total accuracy:  0.640714285714\n",
      "[[ 0.      0.      1.      1.      1.      1.      1.      1.      0.      1.\n",
      "   0.      0.      1.      0.      0.      1.      2.3     3.      3.4\n",
      "   2.35    2.9     3.      2.4     2.8     3.1     2.3     2.9     3.      2.25\n",
      "   2.88    3.      2.3     2.8967  3.125   2.2     2.9     3.25  ]\n",
      " [ 0.      1.      1.      1.      0.      1.      1.      0.      0.      1.\n",
      "   1.      0.      0.      1.      0.      1.      2.4     3.2     3.      2.55\n",
      "   3.4     2.85    2.55    3.4     2.85    2.5     3.1     2.8     2.38\n",
      "   3.2     3.      2.6     3.      2.9     2.5     3.25    3.1   ]]\n",
      "\n",
      "\n",
      "\n",
      "Odds Difference Threshold to remove sample data:  3.1\n",
      "Number of training data under this odds difference threshold:  4923\n",
      "Number of test data under this odds difference threshold:  1178\n",
      "Percentage of predictions with the Prob_threshold of  0.75  :  0.231748726655\n",
      "Accuracy with this prob_threshold:  0.772893772894\n",
      "Total accuracy:  0.657894736842\n",
      "[[ 0.    0.    1.    0.    0.    1.    0.    0.    0.    0.    1.    0.    0.\n",
      "   1.    0.    1.    6.    4.    1.57  6.    4.    1.5   6.5   3.6   1.55\n",
      "   5.    3.6   1.6   6.    3.75  1.57  5.5   4.    1.57  6.5   4.2   1.57]\n",
      " [ 0.    0.    1.    1.    0.    0.    0.    1.    0.    0.    1.    0.    0.\n",
      "   0.    0.    1.    2.    3.3   4.    1.95  3.12  3.8   2.    3.25  3.75\n",
      "   2.    3.2   3.3   1.83  3.2   3.75  1.8   3.25  3.8   1.9   3.25  4.  ]]\n",
      "\n",
      "\n",
      "\n",
      "Odds Difference Threshold to remove sample data:  3.6\n",
      "Number of training data under this odds difference threshold:  4145\n",
      "Number of test data under this odds difference threshold:  1008\n",
      "Percentage of predictions with the Prob_threshold of  0.75  :  0.27876984127\n",
      "Accuracy with this prob_threshold:  0.76512455516\n",
      "Total accuracy:  0.678571428571\n",
      "[[ 0.    0.    0.    0.    1.    1.    1.    0.    0.    0.    0.    0.    0.\n",
      "   1.    0.    0.    2.1   3.3   3.5   2.    3.3   3.6   2.05  3.2   3.6\n",
      "   2.1   3.25  3.4   2.    3.2   3.2   2.1   3.2   3.3   2.05  3.25  3.5 ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    1.    0.    0.    0.    0.    0.\n",
      "   0.    1.    0.    2.3   3.25  3.    2.3   3.35  2.7   2.2   3.3   3.\n",
      "   2.2   3.1   2.8   2.1   3.2   3.    2.15  3.1   3.    2.1   3.2   3.  ]]\n",
      "\n",
      "\n",
      "\n",
      "Odds Difference Threshold to remove sample data:  4.1\n",
      "Number of training data under this odds difference threshold:  3538\n",
      "Number of test data under this odds difference threshold:  856\n",
      "Percentage of predictions with the Prob_threshold of  0.75  :  0.329439252336\n",
      "Accuracy with this prob_threshold:  0.769503546099\n",
      "Total accuracy:  0.692757009346\n",
      "[[ 1.    0.    1.    1.    1.    0.    1.    1.    1.    0.    1.    0.    1.\n",
      "   1.    1.    1.    2.2   3.3   3.25  2.15  3.2   3.3   2.15  3.25  3.25\n",
      "   2.1   3.3   3.1   2.1   3.2   3.    2.15  3.3   3.4   2.2   3.4   3.6 ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    1.    0.    0.    0.    0.    0.\n",
      "   0.    1.    0.    2.3   3.25  3.    2.3   3.35  2.7   2.2   3.3   3.\n",
      "   2.2   3.1   2.8   2.1   3.2   3.    2.15  3.1   3.    2.1   3.2   3.  ]]\n",
      "\n",
      "\n",
      "\n",
      "Odds Difference Threshold to remove sample data:  4.6\n",
      "Number of training data under this odds difference threshold:  2982\n",
      "Number of test data under this odds difference threshold:  723\n",
      "Percentage of predictions with the Prob_threshold of  0.75  :  0.39142461964\n",
      "Accuracy with this prob_threshold:  0.749116607774\n",
      "Total accuracy:  0.709543568465\n",
      "[[ 1.    0.    0.    0.    0.    0.    0.    1.    1.    0.    0.    0.    1.\n",
      "   1.    0.    1.    1.4   4.    7.    1.45  3.9   6.25  1.45  3.75  6.5\n",
      "   1.35  4.    7.    1.4   3.75  7.    1.45  4.    5.5   1.4   3.75  8.  ]\n",
      " [ 1.    0.    0.    0.    0.    1.    1.    1.    1.    0.    0.    0.    1.\n",
      "   0.    0.    0.    2.75  3.3   2.5   2.6   3.3   2.4   2.5   3.3   2.5\n",
      "   2.4   3.2   2.6   2.5   3.25  2.4   2.7   3.4   2.3   2.75  3.3   2.5 ]]\n",
      "\n",
      "\n",
      "\n",
      "Odds Difference Threshold to remove sample data:  5.1\n",
      "Number of training data under this odds difference threshold:  2633\n",
      "Number of test data under this odds difference threshold:  637\n",
      "Percentage of predictions with the Prob_threshold of  0.75  :  0.442700156986\n",
      "Accuracy with this prob_threshold:  0.737588652482\n",
      "Total accuracy:  0.726844583987\n",
      "[[ 1.    0.    1.    0.    0.    0.    1.    0.    1.    0.    1.    0.    1.\n",
      "   1.    1.    1.    1.73  3.6   4.75  1.7   3.75  5.    1.75  3.5   4.6\n",
      "   1.75  3.4   4.3   1.7   3.5   4.5   1.73  3.6   4.75  1.8   3.75  5.  ]\n",
      " [ 1.    0.    1.    0.    0.    0.    1.    0.    1.    0.    1.    0.    1.\n",
      "   1.    0.    1.    2.75  3.2   2.5   2.7   3.15  2.4   2.6   3.2   2.6\n",
      "   2.7   3.    2.5   2.6   3.2   2.37  2.62  2.9   2.5   2.6   3.1   2.35]]\n",
      "\n",
      "\n",
      "\n",
      "Odds Difference Threshold to remove sample data:  5.6\n",
      "Number of training data under this odds difference threshold:  2264\n",
      "Number of test data under this odds difference threshold:  550\n",
      "Percentage of predictions with the Prob_threshold of  0.75  :  0.512727272727\n",
      "Accuracy with this prob_threshold:  0.741134751773\n",
      "Total accuracy:  0.743636363636\n",
      "[[ 0.    0.    0.    0.    1.    1.    1.    0.    0.    0.    0.    0.    0.\n",
      "   1.    0.    0.    2.1   3.3   3.5   2.    3.3   3.6   2.05  3.2   3.6\n",
      "   2.1   3.25  3.4   2.    3.2   3.2   2.1   3.2   3.3   2.05  3.25  3.5 ]\n",
      " [ 1.    0.    1.    0.    0.    0.    1.    0.    1.    0.    1.    0.    1.\n",
      "   1.    1.    1.    1.73  3.6   4.75  1.7   3.75  5.    1.75  3.5   4.6\n",
      "   1.75  3.4   4.3   1.7   3.5   4.5   1.73  3.6   4.75  1.8   3.75  5.  ]]\n",
      "\n",
      "\n",
      "\n",
      "Odds Difference Threshold to remove sample data:  6.1\n",
      "Number of training data under this odds difference threshold:  1923\n",
      "Number of test data under this odds difference threshold:  478\n",
      "Percentage of predictions with the Prob_threshold of  0.75  :  0.556485355649\n",
      "Accuracy with this prob_threshold:  0.751879699248\n",
      "Total accuracy:  0.751046025105\n",
      "[[ 0.    0.    0.    0.    1.    1.    1.    0.    0.    0.    0.    0.    0.\n",
      "   1.    0.    0.    2.1   3.3   3.5   2.    3.3   3.6   2.05  3.2   3.6\n",
      "   2.1   3.25  3.4   2.    3.2   3.2   2.1   3.2   3.3   2.05  3.25  3.5 ]\n",
      " [ 1.    0.    1.    0.    0.    0.    1.    0.    1.    0.    1.    0.    1.\n",
      "   1.    1.    1.    1.73  3.6   4.75  1.7   3.75  5.    1.75  3.5   4.6\n",
      "   1.75  3.4   4.3   1.7   3.5   4.5   1.73  3.6   4.75  1.8   3.75  5.  ]]\n",
      "\n",
      "\n",
      "\n",
      "Odds Difference Threshold to remove sample data:  6.6\n",
      "Number of training data under this odds difference threshold:  1720\n",
      "Number of test data under this odds difference threshold:  419\n",
      "Percentage of predictions with the Prob_threshold of  0.75  :  0.615751789976\n",
      "Accuracy with this prob_threshold:  0.751937984496\n",
      "Total accuracy:  0.749403341289\n",
      "[[ 0.    1.    1.    1.    0.    1.    1.    0.    0.    1.    1.    1.    0.\n",
      "   1.    1.    1.    2.4   3.    3.2   2.2   2.9   3.35  2.4   2.9   3.\n",
      "   2.3   2.9   3.    2.3   2.8   3.    2.25  2.88  3.    2.3   3.    3.  ]\n",
      " [ 0.    0.    0.    0.    1.    1.    1.    0.    0.    0.    0.    0.    0.\n",
      "   1.    0.    0.    2.1   3.3   3.5   2.    3.3   3.6   2.05  3.2   3.6\n",
      "   2.1   3.25  3.4   2.    3.2   3.2   2.1   3.2   3.3   2.05  3.25  3.5 ]]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "\n",
    "clf = LogisticRegression(C=1, n_jobs = -1)\n",
    "# clf.fit(X_train, target_train)\n",
    "\n",
    "# from sklearn.svm import SVC\n",
    "# clf = SVC(C = 1, kernel = 'rbf', probability = True)\n",
    "# clf.fit(X_train, target_train)\n",
    "\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# clf = RandomForestClassifier(min_samples_split = 10, n_estimators = 1000, criterion='entropy')\n",
    "# clf.fit(X_train, target_train)\n",
    "\n",
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "# clf = AdaBoostClassifier(n_estimators = 1000)\n",
    "# clf.fit(X_train, target_train)\n",
    "\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# clf = GaussianNB()\n",
    "# clf.fit(X_train, target_train)\n",
    "\n",
    "for threshold in np.arange(0.1, 7, 0.5):\n",
    "    new_features_train, new_target_train, miss_predicts_rate = correct_predict_based_on_odds(features_train, target_train, threshold)\n",
    "    new_features_test, new_target_test, miss_predicts_rate_test = correct_predict_based_on_odds(features_test, target_test, threshold)\n",
    "    print('Odds Difference Threshold to remove sample data: ', threshold)\n",
    "    print('Number of training data under this odds difference threshold: ', len(new_features_train))\n",
    "    print('Number of test data under this odds difference threshold: ', len(new_features_test))\n",
    "    \n",
    "    # feature scaling\n",
    "    scaler = StandardScaler()  \n",
    "    # Don't cheat - fit only on training data\n",
    "    scaler.fit(new_features_train)  \n",
    "    X_train = scaler.transform(new_features_train)  \n",
    "    # apply same transformation to test data\n",
    "    X_test = scaler.transform(new_features_test)  \n",
    "    \n",
    "    \n",
    "    clf.fit(X_train, new_target_train)\n",
    "    target_predicted_proba = clf.predict_proba(X_test)\n",
    "    # print(target_predicted_proba)\n",
    "    target_predicted = clf.predict(X_test)\n",
    "    prob_threshold = 0.75\n",
    "    \n",
    "    print('Percentage of predictions with the Prob_threshold of ', prob_threshold, \n",
    "          ' : ', np.mean(np.amax(target_predicted_proba, axis = 1) > prob_threshold))\n",
    "    print('Accuracy with this prob_threshold: ', np.mean((target_predicted == new_target_test) & (np.amax(target_predicted_proba, axis = 1) > prob_threshold)) \n",
    "          / np.mean(np.amax(target_predicted_proba, axis = 1) > prob_threshold))\n",
    "    #print(np.mean((target_predicted != new_target_test) & (np.amax(target_predicted_proba, axis = 1) > prob_threshold)))\n",
    "    #print(np.sum((target_predicted == new_target_test) & (np.amax(target_predicted_proba, axis = 1) > prob_threshold)))\n",
    "    #print(np.sum((target_predicted != new_target_test) & (np.amax(target_predicted_proba, axis = 1) > prob_threshold)))\n",
    "    print('Total accuracy: ', accuracy_score(new_target_test, target_predicted))\n",
    "    X_test_greater_than_prob_threshold = features_train[np.amax(target_predicted_proba, axis = 1) > prob_threshold, ]\n",
    "    print(X_test_greater_than_prob_threshold[0 : 2, ])\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0563876651982\n",
      "0.0452276064611\n",
      "0.0111600587372\n",
      "154\n",
      "38\n"
     ]
    }
   ],
   "source": [
    "target_predicted_proba = clf.predict_proba(X_test)\n",
    "# print(target_predicted_proba)\n",
    "target_predicted = clf.predict(X_test)\n",
    "threshold = 0.7\n",
    "print(np.mean(np.amax(target_predicted_proba, axis = 1) > threshold))\n",
    "print(np.mean((target_predicted == target_test) & (np.amax(target_predicted_proba, axis = 1) > threshold)))\n",
    "print(np.mean((target_predicted != target_test) & (np.amax(target_predicted_proba, axis = 1) > threshold)))\n",
    "print(np.sum((target_predicted == target_test) & (np.amax(target_predicted_proba, axis = 1) > threshold)))\n",
    "print(np.sum((target_predicted != target_test) & (np.amax(target_predicted_proba, axis = 1) > threshold)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50748898678414101"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(target_test, target_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first model has around 73% accuracy: this is better than our baseline that always predicts death."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51659324522760641"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(features_test, target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.48441286e+00  -7.28796674e-01   9.30311481e-01  -7.10926690e-01\n",
      "    1.08031987e+00  -9.82817591e-01  -9.63650356e-01  -1.01613843e+00\n",
      "    1.48441286e+00  -7.29859955e-01   9.29350499e-01  -7.14675478e-01\n",
      "    1.08849562e+00   1.02453349e+00   1.03360897e+00  -1.01063074e+00\n",
      "   -6.23852269e-02  -3.90270960e-01  -5.20294382e-01  -3.07370957e-02\n",
      "   -6.19948238e-01  -5.41410701e-01  -3.53839942e-02  -4.49952600e-01\n",
      "   -5.07790115e-01   3.51134814e-02  -5.38890052e-01  -5.54718104e-01\n",
      "   -1.32718927e-04  -4.05296131e-01  -5.91720289e-01  -3.02930688e-02\n",
      "   -5.21776853e-01  -5.67615715e-01  -5.10504542e-02  -4.14046623e-01\n",
      "   -5.58011870e-01]\n",
      " [ -6.73667029e-01  -7.28796674e-01  -1.07490880e+00  -7.10926690e-01\n",
      "    1.08031987e+00  -9.82817591e-01  -9.63650356e-01  -1.01613843e+00\n",
      "   -6.73667029e-01  -7.29859955e-01  -1.07602030e+00  -7.14675478e-01\n",
      "    1.08849562e+00  -9.76053988e-01   1.03360897e+00   9.89481081e-01\n",
      "   -5.74630222e-01   1.54659790e-01   1.11898092e-01  -5.86800407e-01\n",
      "   -1.64227992e-01   3.50720132e-01  -5.99030110e-01  -8.80120677e-02\n",
      "    5.47121316e-01  -5.33943378e-01  -1.04988551e-01   7.54394344e-02\n",
      "   -6.02043230e-01  -2.18364226e-02   2.72189018e-01  -5.69652943e-01\n",
      "   -2.76952882e-01   2.14660139e-01  -5.54674954e-01  -3.63594436e-01\n",
      "    1.14785592e-01]\n",
      " [  1.48441286e+00  -7.28796674e-01  -1.07490880e+00  -7.10926690e-01\n",
      "    1.08031987e+00   1.01748281e+00  -9.63650356e-01   9.84117880e-01\n",
      "   -6.73667029e-01   1.37012586e+00   9.29350499e-01   1.39923648e+00\n",
      "    1.08849562e+00   1.02453349e+00   1.03360897e+00   9.89481081e-01\n",
      "    1.62348438e+00   1.00166715e-01  -9.18011713e-01   1.77646867e+00\n",
      "   -1.07262961e-01  -9.53163394e-01   1.97260529e+00  -8.80120677e-02\n",
      "   -9.60381149e-01   1.82357790e+00  -1.04988551e-01  -9.87951412e-01\n",
      "    1.57523800e+00  -2.18364226e-02  -9.44483256e-01   1.80493144e+00\n",
      "   -3.21289123e-02  -9.12737416e-01   1.58572917e+00  -1.61785686e-01\n",
      "   -8.72938342e-01]\n",
      " [ -6.73667029e-01   1.37212481e+00   9.30311481e-01   1.40661479e+00\n",
      "   -9.25651769e-01   1.01748281e+00   1.03772078e+00  -1.01613843e+00\n",
      "   -6.73667029e-01   1.37012586e+00   9.29350499e-01   1.39923648e+00\n",
      "   -9.18699148e-01   1.02453349e+00   1.03360897e+00   9.89481081e-01\n",
      "   -6.23852269e-02  -7.17229411e-01  -4.22349069e-01  -1.69752924e-01\n",
      "   -7.90843330e-01  -3.52690717e-01  -3.53839942e-02  -8.11893132e-01\n",
      "   -4.73760714e-01  -4.61803557e-02  -8.28157720e-01  -4.36563566e-01\n",
      "   -5.95806706e-02  -9.16575743e-01  -4.47735404e-01  -1.21353827e-01\n",
      "   -7.91083220e-01  -4.42714528e-01  -1.14003517e-01  -6.66307560e-01\n",
      "   -4.57807993e-01]\n",
      " [ -6.73667029e-01  -7.28796674e-01  -1.07490880e+00  -7.10926690e-01\n",
      "    1.08031987e+00   1.01748281e+00   1.03772078e+00  -1.01613843e+00\n",
      "   -6.73667029e-01  -7.29859955e-01  -1.07602030e+00  -7.14675478e-01\n",
      "   -9.18699148e-01   1.02453349e+00  -9.67483861e-01  -1.01063074e+00\n",
      "   -2.56908643e-01  -3.90270960e-01  -3.33307875e-01  -3.08768751e-01\n",
      "   -3.35123084e-01  -2.66908906e-01  -2.81979170e-01  -4.49952600e-01\n",
      "   -2.69584308e-01  -2.08768030e-01  -3.21939301e-01  -2.79024181e-01\n",
      "   -2.82510490e-01  -4.05296131e-01  -3.75742962e-01  -2.26423932e-01\n",
      "   -3.99364867e-01  -3.44108328e-01  -2.71386173e-01  -4.14046623e-01\n",
      "   -3.14659596e-01]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler  \n",
    "scaler = StandardScaler()  \n",
    "# Don't cheat - fit only on training data\n",
    "scaler.fit(features_train)  \n",
    "X_train = scaler.transform(features_train)  \n",
    "# apply same transformation to test data\n",
    "X_test = scaler.transform(features_test)  \n",
    "print(X_train[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'MLPClassifier'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-f2425f4dcc55>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneural_network\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMLPClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# clf = MLPClassifier(algorithm='l-bfgs', alpha=1e-5, hidden_layer_sizes=(15,), random_state=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# clf.fit(X_train, target_train)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'MLPClassifier'"
     ]
    }
   ],
   "source": [
    "#from sklearn.neural_network import MLPClassifier # this algorithm is not released yet, only in dev version\n",
    "\n",
    "# clf = MLPClassifier(algorithm='l-bfgs', alpha=1e-5, hidden_layer_sizes=(15,), random_state=1)\n",
    "# clf.fit(X_train, target_train)\n",
    "\n",
    "# target_predicted = clf.predict(X_test)\n",
    "# print(np.mean(target_predicted == target_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation and interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting linear model weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `coef_` attribute of a fitted linear model such as `LogisticRegression` holds the weights of each features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['bit0', 'bit1', 'bit2', 'bit3', 'bit4', 'bit5', 'bit6', 'bit7', 'bit8',\n",
       "       'bit9', 'bit10', 'bit11', 'bit12', 'bit13', 'bit14', 'bit15', 'B365H',\n",
       "       'B365D', 'B365A', 'BWH', 'BWD', 'BWA', 'GBH', 'GBD', 'GBA', 'IWH',\n",
       "       'IWD', 'IWA', 'LBH', 'LBD', 'LBA', 'WHH', 'WHD', 'WHA', 'VCH', 'VCD',\n",
       "       'VCA'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = numerical_features.columns\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.76808623, -0.40818026, -0.29203066, -0.02932457, -0.02199954,\n",
       "        -0.03508011, -0.0689908 , -0.02618309,  0.86282651,  0.4390244 ,\n",
       "         0.34793201, -0.02148758,  0.0076466 ,  0.02010884,  0.01454142,\n",
       "        -0.01472004, -0.18737633,  0.08501214,  0.01768293,  0.1155593 ,\n",
       "         0.00590247,  0.0386236 , -0.52307762,  0.13835045,  0.01614931,\n",
       "         0.00118032,  0.11468958, -0.03474033,  0.27968884, -0.13250774,\n",
       "        -0.00637548, -0.01658148,  0.00913953,  0.11997202, -0.02119795,\n",
       "        -0.01283112, -0.03411416],\n",
       "       [ 0.58154815,  0.39006421,  0.24981341,  0.06797965,  0.01790498,\n",
       "         0.06487292,  0.0551522 , -0.04735843, -0.71345043, -0.42278669,\n",
       "        -0.26872829,  0.10472641, -0.00944496,  0.00337421,  0.01967259,\n",
       "         0.01603245, -0.05755316, -0.04909074, -0.015547  ,  0.12216303,\n",
       "         0.0972385 , -0.08179183, -0.06536372, -0.39820286,  0.08099176,\n",
       "        -0.08313485, -0.19509855,  0.05498296,  0.04061658, -0.12430556,\n",
       "         0.01552817, -0.06327311, -0.06991077, -0.09731784,  0.0879096 ,\n",
       "         0.19546559,  0.05159916],\n",
       "       [ 0.31338676,  0.08052779,  0.08945123, -0.03673266,  0.0135366 ,\n",
       "        -0.02557452,  0.02185869,  0.08412734, -0.30831025, -0.0862064 ,\n",
       "        -0.13313706, -0.08912616,  0.00173115, -0.03518344, -0.04444749,\n",
       "         0.00244543,  0.1474027 , -0.06419382, -0.03670389, -0.13646244,\n",
       "        -0.10848224,  0.08307141,  0.34298109,  0.20386662, -0.2436134 ,\n",
       "         0.08153986,  0.11239611,  0.00097799, -0.19096219,  0.34535072,\n",
       "         0.04949899,  0.04783993,  0.05055927, -0.10055781, -0.07223278,\n",
       "        -0.26017682, -0.02680318]])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# x = np.arange(len(feature_names))\n",
    "# plt.bar(x, logreg.coef_.ravel())\n",
    "# plt.xticks(x + 0.5, feature_names, rotation=30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, survival is slightly positively linked with Fare (the higher the fare, the higher the likelyhood the model will predict survival) while passenger from first class and lower ages are predicted to survive more often than older people from the 3rd class.\n",
    "\n",
    "First-class cabins were closer to the lifeboats and children and women reportedly had the priority. Our model seems to capture that historical data. We will see later if the sex of the passenger can be used as an informative predictor to increase the predictive accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to see the details of the false positive and false negative errors by computing the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1376   19  167]\n",
      " [ 709   27  176]\n",
      " [ 551   19  361]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(target_test, target_predicted)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The true labeling are seen as the rows and the predicted labels are the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUYAAAEaCAYAAABtm/58AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nI5T4AwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6fa4b46f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_confusion(cm, target_names = ['home win', 'home draw', 'home lose'],\n",
    "                   title='Confusion matrix'):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    tick_marks = np.arange(len(target_names))\n",
    "    plt.xticks(tick_marks, target_names, rotation=60)\n",
    "    plt.yticks(tick_marks, target_names)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    # Convenience function to adjust plot parameters for a clear layout.\n",
    "    plt.tight_layout()\n",
    "    \n",
    "plot_confusion(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1376   19  167]\n",
      " [ 709   27  176]\n",
      " [ 551   19  361]]\n"
     ]
    }
   ],
   "source": [
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can normalize the number of prediction by dividing by the total number of true \"survived\" and \"not survived\" to compute true and false positive rates for survival in the first row and the false negative and true negative rates in the second row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1562,  912,  931])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.8809219   0.01216389  0.10691421]\n",
      " [ 0.77741228  0.02960526  0.19298246]\n",
      " [ 0.59183673  0.02040816  0.3877551 ]]\n"
     ]
    }
   ],
   "source": [
    "cm_normalized = cm.astype(np.float64) / cm.sum(axis=1)[:, np.newaxis]\n",
    "print(cm_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT0AAAEaCAYAAABjMoATAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wGhgqqr7FmSRQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6fa4af2eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion(cm_normalized, title=\"Normalized confusion matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can therefore observe that the fact that the target classes are not balanced in the dataset makes the accuracy score not very informative.\n",
    "\n",
    "scikit-learn provides alternative classification metrics to evaluate models performance on imbalanced data such as precision, recall and f1 score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   home win       0.52      0.88      0.66      1562\n",
      "  home draw       0.42      0.03      0.06       912\n",
      "  home lose       0.51      0.39      0.44       931\n",
      "\n",
      "avg / total       0.49      0.52      0.44      3405\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(target_test, target_predicted,\n",
    "                            target_names=['home win', 'home draw', 'home lose']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to quantify the quality of a binary classifier on imbalanced data is to compute the precision, recall and f1-score of a model (at the default fixed decision threshold of 0.5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression is a probabilistic models: instead of just predicting a binary outcome (survived or not) given the input features it can also estimates the posterior probability of the outcome given the input features using the `predict_proba` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.54902252,  0.27728864,  0.17368883],\n",
       "       [ 0.47090786,  0.2654256 ,  0.26366654],\n",
       "       [ 0.31957595,  0.29024755,  0.39017651],\n",
       "       [ 0.72126957,  0.17667731,  0.10205312],\n",
       "       [ 0.33661806,  0.3271818 ,  0.33620014]])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_predicted_proba = logreg.predict_proba(features_test)\n",
    "target_predicted_proba[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the decision threshold is 0.5: if we vary the decision threshold from 0 to 1 we could generate a family of binary classifier models that address all the possible trade offs between false positive and false negative prediction errors.\n",
    "\n",
    "We can summarize the performance of a binary classifier for all the possible thresholds by plotting the ROC curve and quantifying the Area under the ROC curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import roc_curve\n",
    "# from sklearn.metrics import auc\n",
    "\n",
    "# def plot_roc_curve(target_test, target_predicted_proba):\n",
    "#     fpr, tpr, thresholds = roc_curve(target_test, target_predicted_proba[:, 1])\n",
    "    \n",
    "#     roc_auc = auc(fpr, tpr)\n",
    "#     # Plot ROC curve\n",
    "#     plt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc)\n",
    "#     plt.plot([0, 1], [0, 1], 'k--')  # random predictions curve\n",
    "#     plt.xlim([0.0, 1.0])\n",
    "#     plt.ylim([0.0, 1.0])\n",
    "#     plt.xlabel('False Positive Rate or (1 - Specifity)')\n",
    "#     plt.ylabel('True Positive Rate or (Sensitivity)')\n",
    "#     plt.title('Receiver Operating Characteristic')\n",
    "#     plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot_roc_curve(target_test, target_predicted_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the area under ROC curve is 0.756 which is very similar to the accuracy (0.732). However the ROC-AUC score of a random model is expected to 0.5 on average while the accuracy score of a random model depends on the class imbalance of the data. ROC-AUC can be seen as a way to callibrate the predictive accuracy of a model against class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We previously decided to randomly split the data to evaluate the model on 20% of held-out data. However the location randomness of the split might have a significant impact in the estimated accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51806167400881054"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features_array, target, test_size=0.20, random_state=0)\n",
    "\n",
    "logreg.fit(features_train, target_train).score(features_test, target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54537444933920709"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features_array, target, test_size=0.20, random_state=1)\n",
    "\n",
    "logreg.fit(features_train, target_train).score(features_test, target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52922173274596185"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features_array, target, test_size=0.20, random_state=2)\n",
    "\n",
    "logreg.fit(features_train, target_train).score(features_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So instead of using a single train / test split, we can use a group of them and compute the min, max and mean scores as an estimation of the real test score while not underestimating the variability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.51086318,  0.51189427,  0.50837004,  0.51909518,  0.51777843])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "scores = cross_val_score(logreg, features_array, target, cv=5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.50837004405286346, 0.51360022254690818, 0.51909518213866035)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.min(), scores.mean(), scores.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cross_val_score` reports accuracy by default be it can also be used to report other performance metrics such as ROC-AUC or f1-score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# scores = cross_val_score(logreg, features_array, target, cv=5,\n",
    "#                          scoring='roc_auc')\n",
    "# scores.min(), scores.mean(), scores.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**:\n",
    "\n",
    "- Compute cross-validated scores for other classification metrics ('precision', 'recall', 'f1', 'accuracy'...).\n",
    "\n",
    "- Change the number of cross-validation folds between 3 and 10: what is the impact on the mean score? on the processing time?\n",
    "\n",
    "Hints:\n",
    "\n",
    "The list of classification metrics is available in the online documentation:\n",
    "\n",
    "  http://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values\n",
    "  \n",
    "You can use the `%%time` cell magic on the first line of an IPython cell to measure the time of the execution of the cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ares/.local/lib/python3.4/site-packages/sklearn/metrics/metrics.py:1771: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ares/.local/lib/python3.4/site-packages/sklearn/metrics/metrics.py:1771: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ares/.local/lib/python3.4/site-packages/sklearn/metrics/metrics.py:1771: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ares/.local/lib/python3.4/site-packages/sklearn/metrics/metrics.py:1771: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ares/.local/lib/python3.4/site-packages/sklearn/metrics/metrics.py:1771: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ares/.local/lib/python3.4/site-packages/sklearn/metrics/metrics.py:1771: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ares/.local/lib/python3.4/site-packages/sklearn/metrics/metrics.py:1771: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ares/.local/lib/python3.4/site-packages/sklearn/metrics/metrics.py:1771: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ares/.local/lib/python3.4/site-packages/sklearn/metrics/metrics.py:1771: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ares/.local/lib/python3.4/site-packages/sklearn/metrics/metrics.py:1771: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ares/.local/lib/python3.4/site-packages/sklearn/metrics/metrics.py:1771: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ares/.local/lib/python3.4/site-packages/sklearn/metrics/metrics.py:1771: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.438429675336\n",
      "CPU times: user 54.6 s, sys: 443 µs, total: 54.6 s\n",
      "Wall time: 54.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "scores = cross_val_score(logreg, features_array, target, cv=30, scoring = 'f1')\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More feature engineering and richer models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now try to build richer models by including more features as potential predictors for our model.\n",
    "\n",
    "Categorical variables such as `data['Embarked']` or `data['Sex']` can be converted as boolean indicators features also known as dummy variables or one-hot-encoded features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pd.get_dummies(data['Sex'], prefix='Sex').head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pd.get_dummies(data.Embarked, prefix='Embarked').head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine those new numerical features with the previous features using `pandas.concat` along `axis=1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rich_features = pd.concat([data[['Fare', 'Pclass', 'Age']],\n",
    "#                            pd.get_dummies(data['Sex'], prefix='Sex'),\n",
    "#                            pd.get_dummies(data['Embarked'], prefix='Embarked')],\n",
    "#                           axis=1)\n",
    "# rich_features.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By construction the new `Sex_male` feature is redundant with `Sex_female`. Let us drop it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rich_features_no_male = rich_features.drop('Sex_male', 1)\n",
    "# rich_features_no_male.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us not forget to imput the median age for passengers without age information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rich_features_final = rich_features_no_male.fillna(rich_features_no_male.dropna().median())\n",
    "# rich_features_final.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can finally cross-validate a logistic regression model on this new data an observe that the mean score has significantly increased:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "# logreg = LogisticRegression(C=1)\n",
    "# scores = cross_val_score(logreg, rich_features_final, target, cv=5, scoring='accuracy')\n",
    "# print(\"Logistic Regression CV scores:\")\n",
    "# print(\"min: {:.3f}, mean: {:.3f}, max: {:.3f}\".format(\n",
    "#     scores.min(), scores.mean(), scores.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**:\n",
    "\n",
    "- change the value of the parameter `C`. Does it have an impact on the score?\n",
    "\n",
    "- fit a new instance of the logistic regression model on the full dataset.\n",
    "\n",
    "- plot the weights for the features of this newly fitted logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # %load solutions/04A_plot_logistic_regression_weights.py\n",
    "# logreg_new = LogisticRegression(C=1).fit(rich_features_final, target)\n",
    "\n",
    "# feature_names = rich_features_final.columns.values\n",
    "# x = np.arange(len(feature_names))\n",
    "# plt.bar(x, logreg_new.coef_.ravel())\n",
    "# _ = plt.xticks(x + 0.5, feature_names, rotation=30)\n",
    "\n",
    "# # Rich young women like Kate Winslet tend to survive the Titanic better\n",
    "# # than poor men like Leonardo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "# logreg = LogisticRegression(C=0.1)\n",
    "# scores = cross_val_score(logreg, rich_features_final, target, cv=5, scoring='accuracy')\n",
    "# #print(logreg.coef_)\n",
    "# print(\"Logistic Regression CV scores:\")\n",
    "# print(\"min: {:.3f}, mean: {:.3f}, max: {:.3f}\".format(\n",
    "#     scores.min(), scores.mean(), scores.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Non-linear models: ensembles of randomized trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sklearn` also implement non linear models that are known to perform very well for data-science projects where datasets have not too many features (e.g. less than 5000).\n",
    "\n",
    "In particular let us have a look at Random Forests and Gradient Boosted Trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest CV scores:\n",
      "min: 0.453, mean: 0.476, max: 0.506\n",
      "CPU times: user 198 ms, sys: 120 ms, total: 318 ms\n",
      "Wall time: 12.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "scores = cross_val_score(rf, features_array, target, cv=5, n_jobs=4,\n",
    "                         scoring='accuracy')\n",
    "print(\"Random Forest CV scores:\")\n",
    "print(\"min: {:.3f}, mean: {:.3f}, max: {:.3f}\".format(\n",
    "    scores.min(), scores.mean(), scores.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosted Trees CV scores:\n",
      "min: 0.470, mean: 0.498, max: 0.521\n",
      "CPU times: user 206 ms, sys: 148 ms, total: 354 ms\n",
      "Wall time: 42.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1,\n",
    "                                subsample=.8, max_features=.5)\n",
    "scores = cross_val_score(gb, features_array, target, cv=5, n_jobs=4,\n",
    "                         scoring='accuracy')\n",
    "print(\"Gradient Boosted Trees CV scores:\")\n",
    "print(\"min: {:.3f}, mean: {:.3f}, max: {:.3f}\".format(\n",
    "    scores.min(), scores.mean(), scores.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both models seem to do slightly better than the logistic regression model on this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**:\n",
    "\n",
    "- Change the value of the learning_rate and other `GradientBoostingClassifier` parameter, can you get a better mean score?\n",
    "\n",
    "- Would treating the `PClass` variable as categorical improve the models performance?\n",
    "\n",
    "- Find out which predictor variables (features) are the most informative for those models.\n",
    "\n",
    "Hints:\n",
    "\n",
    "Fitted ensembles of trees have `feature_importances_` attribute that can be used similarly to the `coef_` attribute of linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # %load solutions/04B_more_categorical_variables.py\n",
    "# features = pd.concat([data.get(['Fare', 'Age']),\n",
    "#                       pd.get_dummies(data.Sex, prefix='Sex'),\n",
    "#                       pd.get_dummies(data.Pclass, prefix='Pclass'),\n",
    "#                       pd.get_dummies(data.Embarked, prefix='Embarked')],\n",
    "#                      axis=1)\n",
    "# features = features.drop('Sex_male', 1)\n",
    "# features = features.fillna(features.dropna().median())\n",
    "# features.head(5)\n",
    "\n",
    "\n",
    "# logreg = LogisticRegression(C=1)\n",
    "# scores = cross_val_score(logreg, features, target, cv=5, scoring='accuracy')\n",
    "# print(\"Logistic Regression CV scores:\")\n",
    "# print(\"min: {:.3f}, mean: {:.3f}, max: {:.3f}\".format(\n",
    "#     scores.min(), scores.mean(), scores.max()))\n",
    "\n",
    "# gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1,\n",
    "#                                 subsample=.8, max_features=.5)\n",
    "# scores = cross_val_score(gb, features, target, cv=5, n_jobs=4,\n",
    "#                          scoring='accuracy')\n",
    "# print(\"Gradient Boosting Trees CV scores:\")\n",
    "# print(\"min: {:.3f}, mean: {:.3f}, max: {:.3f}\".format(\n",
    "#     scores.min(), scores.mean(), scores.max()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# features = pd.concat([data.get(['Fare', 'Age']),\n",
    "#                       pd.get_dummies(data.Sex, prefix='Sex'),\n",
    "#                       pd.get_dummies(data.Pclass, prefix='Pclass'),\n",
    "#                       pd.get_dummies(data.Embarked, prefix='Embarked')],\n",
    "#                      axis=1)\n",
    "# features = features.drop('Sex_male', 1)\n",
    "# features = features.fillna(features.dropna().median())\n",
    "# features.head(5)\n",
    "\n",
    "\n",
    "# logreg = LogisticRegression(C=1)\n",
    "# scores = cross_val_score(logreg, features, target, cv=5, scoring='accuracy')\n",
    "# print(\"Logistic Regression CV scores:\")\n",
    "# print(\"min: {:.3f}, mean: {:.3f}, max: {:.3f}\".format(\n",
    "#     scores.min(), scores.mean(), scores.max()))\n",
    "\n",
    "# gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1,\n",
    "#                                 subsample=.8, max_features=.5)\n",
    "# scores = cross_val_score(gb, features, target, cv=5, n_jobs=4,\n",
    "#                          scoring='accuracy')\n",
    "# print(\"Gradient Boosting Trees CV scores:\")\n",
    "# print(\"min: {:.3f}, mean: {:.3f}, max: {:.3f}\".format(\n",
    "#     scores.min(), scores.mean(), scores.max()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # %load solutions/04C_feature_importance.py\n",
    "# gb_new = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1,\n",
    "#                                     subsample=.8, max_features=.5)\n",
    "# gb_new.fit(features, target)\n",
    "# feature_names = features.columns.values\n",
    "# x = np.arange(len(feature_names))\n",
    "# plt.bar(x, gb_new.feature_importances_)\n",
    "# _ = plt.xticks(x + 0.5, feature_names, rotation=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gb_new = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1,\n",
    "#                                     subsample=.8, max_features=.5)\n",
    "# gb_new.fit(features, target)\n",
    "# feature_names = features.columns.values\n",
    "# x = np.arange(len(feature_names))\n",
    "# plt.bar(x, gb_new.feature_importances_)\n",
    "# _ = plt.xticks(x + 0.5, feature_names, rotation=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of changing the value of the learning rate manually and re-running the cross-validation, we can find the best values for the parameters automatically (assuming we are ready to wait):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# gb = GradientBoostingClassifier(n_estimators=100, subsample=.8)\n",
    "\n",
    "# params = {\n",
    "#     'learning_rate': [0.05, 0.1, 0.5],\n",
    "#     'max_features': [0.5, 1],\n",
    "#     'max_depth': [3, 4, 5],\n",
    "# }\n",
    "# gs = GridSearchCV(gb, params, cv=5, scoring='roc_auc', n_jobs=4)\n",
    "# gs.fit(features_array, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us sort the models by mean validation score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sorted(gs.grid_scores_, key=lambda x: x.mean_validation_score, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should note that the mean scores are very close to one another and almost always within one standard deviation of one another. This means that all those parameters are quite reasonable. The only parameter of importance seems to be the `learning_rate`: 0.5 seems to be a bit too high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoiding data snooping with pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When doing imputation in pandas, prior to computing the train test split we use data from the test to improve the accuracy of the median value that we impute on the training set. This is actually cheating. To avoid this we should compute the median of the features on the training fold and use that median value to do the imputation both on the training and validation fold for a given CV split.\n",
    "\n",
    "To do this we can prepare the features as previously but without the imputation: we just replace missing values by the -1 marker value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fare</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.2500</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>71.2833</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.9250</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53.1000</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.0500</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Fare  Age  Sex_female  Pclass_1  Pclass_2  Pclass_3  Embarked_C  \\\n",
       "0   7.2500   22           0         0         0         1           0   \n",
       "1  71.2833   38           1         1         0         0           1   \n",
       "2   7.9250   26           1         0         0         1           0   \n",
       "3  53.1000   35           1         1         0         0           0   \n",
       "4   8.0500   35           0         0         0         1           0   \n",
       "\n",
       "   Embarked_Q  Embarked_S  \n",
       "0           0           1  \n",
       "1           0           0  \n",
       "2           0           1  \n",
       "3           0           1  \n",
       "4           0           1  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = pd.concat([data[['Fare', 'Age']],\n",
    "                      pd.get_dummies(data['Sex'], prefix='Sex'),\n",
    "                      pd.get_dummies(data['Pclass'], prefix='Pclass'),\n",
    "                      pd.get_dummies(data['Embarked'], prefix='Embarked')],\n",
    "                     axis=1)\n",
    "features = features.drop('Sex_male', 1)\n",
    "\n",
    "# Because of the following bug we cannot use NaN as the missing\n",
    "# value marker, use a negative value as marker instead:\n",
    "# https://github.com/scikit-learn/scikit-learn/issues/3044\n",
    "features = features.fillna(-1)\n",
    "features.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the `Imputer` transformer of scikit-learn to find the median value on the training set and apply it on missing values of both the training set and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features.values, target, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Imputer(axis=0, copy=True, missing_values=-1, strategy='median', verbose=0)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "imputer = Imputer(strategy='median', missing_values=-1)\n",
    "\n",
    "imputer.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The median age computed on the training set is stored in the `statistics_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 14.5,  29. ,   0. ,   0. ,   0. ,   1. ,   0. ,   0. ,   1. ])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Fare', 'Age', 'Sex_female', 'Pclass_1', 'Pclass_2', 'Pclass_3',\n",
       "       'Embarked_C', 'Embarked_Q', 'Embarked_S'], dtype=object)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputation can now happen by calling  the transform method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_imputed = imputer.transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(X_train == -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(X_train_imputed == -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(X_test == -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(X_test_imputed == -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use a pipeline that wraps an imputer transformer and the classifier itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.798882681564 0.823857227514 0.848314606742\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "imputer = Imputer(strategy='median', missing_values=-1)\n",
    "\n",
    "classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1,\n",
    "                                        subsample=.8, max_features=.5,\n",
    "                                        random_state=0)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('imp', imputer),\n",
    "    ('clf', classifier),\n",
    "])\n",
    "\n",
    "scores = cross_val_score(pipeline, features.values, target, cv=5, n_jobs=4,\n",
    "                         scoring='accuracy', )\n",
    "print(scores.min(), scores.mean(), scores.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean cross-validation is slightly lower than we used the imputation on the whole data as we did earlier although not by much. This means that in this case the data-snooping was not really helping the model cheat by much.\n",
    "\n",
    "Let us re-run the grid search, this time on the pipeline. Note that thanks to the pipeline structure we can optimize the interaction of the imputation method with the parameters of the downstream classifier without cheating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 425 ms, sys: 46.7 ms, total: 471 ms\n",
      "Wall time: 3.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "params = {\n",
    "    'imp__strategy': ['mean', 'median'],\n",
    "    'clf__max_features': [0.5, 1],\n",
    "    'clf__max_depth': [3, 4, 5],\n",
    "}\n",
    "gs = GridSearchCV(pipeline, params, cv=5, scoring='roc_auc', n_jobs=4)\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: 0.86740, std: 0.02681, params: {'clf__max_features': 0.5, 'clf__max_depth': 3, 'imp__strategy': 'median'},\n",
       " mean: 0.86680, std: 0.02821, params: {'clf__max_features': 0.5, 'clf__max_depth': 5, 'imp__strategy': 'median'},\n",
       " mean: 0.86583, std: 0.02757, params: {'clf__max_features': 0.5, 'clf__max_depth': 4, 'imp__strategy': 'mean'},\n",
       " mean: 0.86545, std: 0.02961, params: {'clf__max_features': 0.5, 'clf__max_depth': 4, 'imp__strategy': 'median'},\n",
       " mean: 0.86477, std: 0.02559, params: {'clf__max_features': 0.5, 'clf__max_depth': 3, 'imp__strategy': 'mean'},\n",
       " mean: 0.85962, std: 0.02592, params: {'clf__max_features': 1, 'clf__max_depth': 4, 'imp__strategy': 'mean'},\n",
       " mean: 0.85915, std: 0.02524, params: {'clf__max_features': 1, 'clf__max_depth': 3, 'imp__strategy': 'median'},\n",
       " mean: 0.85886, std: 0.03214, params: {'clf__max_features': 0.5, 'clf__max_depth': 5, 'imp__strategy': 'mean'},\n",
       " mean: 0.85656, std: 0.02388, params: {'clf__max_features': 1, 'clf__max_depth': 3, 'imp__strategy': 'mean'},\n",
       " mean: 0.85557, std: 0.02498, params: {'clf__max_features': 1, 'clf__max_depth': 4, 'imp__strategy': 'median'},\n",
       " mean: 0.85482, std: 0.02607, params: {'clf__max_features': 1, 'clf__max_depth': 5, 'imp__strategy': 'mean'},\n",
       " mean: 0.85471, std: 0.02594, params: {'clf__max_features': 1, 'clf__max_depth': 5, 'imp__strategy': 'median'}]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(gs.grid_scores_, key=lambda x: x.mean_validation_score, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86740320952841032"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEZCAYAAACTsIJzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0b71add470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_roc_curve(y_test, gs.predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__max_depth': 3, 'clf__max_features': 0.5, 'imp__strategy': 'median'}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this search we can conclude that the imputation by the 'mean' strategy is generally a slightly better imputation strategy when training a GBRT model on this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further integrating sklearn and pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper tool for better sklearn / pandas integration: https://github.com/paulgb/sklearn-pandas by making it possible to embed the feature construction from the raw dataframe directly inside a pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to:\n",
    "\n",
    "- Kaggle for setting up the Titanic challenge.\n",
    "\n",
    "- This blog post by Philippe Adjiman for inspiration:\n",
    "\n",
    "http://www.philippeadjiman.com/blog/2013/09/12/a-data-science-exploration-from-the-titanic-in-r/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
